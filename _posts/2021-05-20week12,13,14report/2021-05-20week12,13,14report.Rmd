---
title: "WEEK_12,13,14 Report"
description: |
  Deep learning with Torch
author:
  - name: sonsungman
    url: https://sonsungman.github.io/SKKU-Preditive.m/
date: 05-20-2021
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
library(purrr) ; library(magrittr) ; library(MASS)
```

```{r load_lib, include=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
library(kernlab)
library(recipes)
library(torch)
library(palmerpenguins)
library(tidyverse)
theme_set(theme_bw())
```

### Introduction

The use of neural networks, generally referred to as 'Deep Learning', in the Actuarial Science is increasing.
Claim reserving, non-life pricing, mortality modelling, and telematics are the examples. In this report, Using a package called torch, I try to use deep learning in R.

***

A list of the books I have referenced to produce this report is shown below.

> -  AI in Actuarial Science, Ronald Richman(2018)
> -  https://deeplearning-playbook.netlify.app/ 딥러닝 공략집, 슬기로운 통계 생활(2021)


***

Deep learning has great advantages in image classification. It also has high predictive performance compared to various models prior to deep learning. However, more research seems to be needed on the interpretive aspects of previous traditional models.


### Example from Lecture

```{r}

#penguins %>% head(5)




#penguin_data <- penguins %>% 
#  recipe(species ~ .) %>%
#  step_impute_mode(all_nominal()) %>% 
#  step_impute_mean(all_numeric()) %>% 
#  step_dummy(all_nominal(), -all_outcomes()) %>% 
#  step_integer(all_nominal()) %>% 
#  step_normalize(all_predictors(), -all_outcomes()) %>% 
#  prep() %>% juice()

#penguin_data %>% dim()

#penguin_data %>% head()

#library(rsample)
#split_data <- initial_split(penguin_data, prop = 0.8) 
#penguin_data_train <- training(split_data)
#penguin_data_test <- testing(split_data)

#TwoLayerNet <- nn_module(
#  classname = "TowLayerNet",
#  initialize = function(data_in, hidden, data_out){
#    self$hidden1 <- nn_linear(data_in, hidden)
#    self$output_layer <- nn_linear(hidden, data_out)
#    self$tanh <- nn_tanh()
#  },
#  forward = function(X) {
#    x <- self$tanh(self$hidden1(X))
#    y_hat <- self$output_layer(x)
#    return(y_hat)
#  }
#)

#my_net <- TwoLayerNet(8, 10, 3)


#penguin_dataset <- dataset(
#  name = "penguin_data",
#  initialize = function() {
#    self$data <- torch_tensor(as.matrix(select(penguin_data, species, #everything())))
#  },
#  .getitem = function(index) {
#    x <- self$data[index, 2:9]
#    y <- self$data[index, 1]
#    
#    list(x, y)
#  },
#  .length = function() {
#    self$data$size()[[1]]
#  }
#)

#torch_penguin_data <- penguin_dataset()
#torch_penguin_data

#torch_penguin_data$.getitem(1:6)
#penguin_dl <- dataloader(torch_penguin_data, batch_size = 8)
#penguin_dl$.length()

#b <- penguin_dl$.iter()$.next()
#length(b)
#b[[1]]

#device <- if (cuda_is_available()) torch_device("cuda:0") else "cpu"
#my_net <- my_net$to(device = device)

#criterion <- nn_cross_entropy_loss()
#optimizer <- optim_sgd(my_net$parameters, 
#                       lr = 0.1, momentum = 0.9)
#num_epochs <- 5

#train_batch <- function(b) {
  
#  optimizer$zero_grad()
#  output <- my_net(b[[1]])
#  loss <- criterion(output, b[[2]]$to(device = device,
#                                      dtype  = torch_long()))
#  loss$backward()
#  optimizer$step()
#  loss$item()
#  
#}

#valid_batch <- function(b) {
  
#  output <- my_net(b[[1]])
#  loss <- criterion(output, b[[2]]$to(device = device, 
#                                      dtype  = torch_long()))
#  loss$item()
#}

#for (epoch in 1:num_epochs) {
#  my_net$train()
#  train_losses <- c()  
#  
#  coro::loop(for (b in penguin_dl) {
#    loss <- train_batch(b)
#    train_losses <- c(train_losses, loss)
#  })
#  
#  my_net$eval()
#  valid_losses <- c()
#  
#  coro::loop(for (b in valid_dl) {
#    loss <- valid_batch(b)
#    valid_losses <- c(valid_losses, loss)
#  })
#  
#  if (epoch %% 100 == 0){
#    cat(sprintf("\nLoss at epoch %d: training: %3f, validation: %3f\n",
#                epoch, mean(train_losses), mean(valid_losses)))
#  }
#  
#}

```

