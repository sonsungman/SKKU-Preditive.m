---
title: "WEEK_6 Report"
description: |
  Lasso, Ridge, and Elastic Net with hyperparameter tuning
author:
  - name: sonsungman
    url: https://sonsungman.github.io/SKKU-Preditive.m/
date: 04-02-2021
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    draft: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
library(purrr) ; library(magrittr) ; library(MASS)
```

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
theme_set(theme_bw())
```


# Introduction


The Lasso and Ridge models impose penalties when estimating the coefficients of multiple linear regression models. This penalty imposition reduces the estimated coefficient value, thus solving the overfitting problem. In the case of Ridge, the multicollinearity problem can be solved because no variable selection is made, but the estimated coefficient value with less impact is close to zero. On the other hand, in the case of Lasso, the choice of variables is naturally made through penalty imposition. The model that uses both Lasso and Ridge's penalty charging method is the Elastic Net model.

The purpose of this report is to conduct tuning of hyperparameters corresponding to lambda and alpha in the Elastic Net model using cross-validation, to check the cross-validation error, to learn the Elastic Net model and to get the results. Also, I check out the Outlier and remove it.



***

A list of the books I have referenced to produce this report is shown below.

> -  Feature Engineering and Selection_ A Practical Approach for Predictive Models, Max Kuhn, Kjell Johnson(2019)
> -  An Introduction to Statistical Learning with Applications in R , Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013)
> -  Applied Predictive Modeling, Max Kuhn, Kjell Johnson (2013)
> -  R for Data Science_ Import, Tidy, Transform,  Visualize, and Model Data , Hadley Wickham, Garrett  Grolemund (2017)
> - Study Manual Exam PA, Ambrose Lo (2020 Spring)

***



# Example with Lecture


## Preparations {.tabset .tabset-fade}

### Data load


```{r, message=FALSE}
train <- read_csv(file= "train.csv")
test <- read_csv("test.csv")
```


### Data overview {.tabset .tabset-fade}

```{r}
dim(train)
dim(test)
```

We can see train doesn't have the target variable `SalePrice`.

```{r}
"SalePrice" %in% names(test)
```


### Detailed info. `train`

```{r}
skim(train)
```

### Detailed info. `test`

```{r}
skim(test)
```


```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = SalePrice)) +
  geom_histogram()
```

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = log(SalePrice))) +
  geom_histogram()
```

### `NA`s

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(naniar)
train %>% 
  # select_if(~sum(is.na(.)) > 0) %>% # alternative way
  select(where(~sum(is.na(.)) > 0)) %>% 
  gg_miss_var()
```


```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  select(where(~sum(is.na(.)) > 0)) %>% 
  gg_miss_upset()
```

### Preprecessing with `recipe`

### `all_data` combine and name cleaning with `janitor`

```{r}
all_data <- bind_rows(train, test) %>% 
  janitor::clean_names()
names(all_data)[1:10]
```

### Make recipe

```{r}
housing_recipe <- all_data %>% 
  recipe(sale_price ~ .) %>%
  step_rm(id) %>% 
  step_log(sale_price) %>% 
  step_dummy(all_nominal()) %>% 
  step_meanimpute(all_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  prep(training = all_data)

print(housing_recipe)
```

### `juice` the all_data2 and split

```{r}
all_data2 <- juice(housing_recipe)
```

We are done for preprocessing. Let's split the data set.

```{r}
  train_index <- seq_len(nrow(train))
  train2 <- all_data2[train_index,]
  test2 <- all_data2[-train_index,]
```


```{r}
train2 %>% 
  head() %>% 
  kable()
```


Using cross-validation, we now want to determine the optimal hyperparameters. There are alpha and lambda in the hyperparameters. The expressions below are called hyperparameters because $\alpha$ and $\lambda$ are established prior to determining the coefficients through model learning. $\lambda$ is concerned with the degree of penalty and $\alpha$ is concerned with the proportion of penalty in Lasso and Ridge respectively.

$$\underset{\beta}{min}\left(y-X\beta\right)^{T}\left(y-X\beta\right)+\frac{\lambda}{2}\left(\alpha\left\Vert \beta\right\Vert _{1}+\left(1-\alpha\right)\left\Vert \beta\right\Vert _{2}^{2}\right)$$

# Split the train into validation and train

```{r}
set.seed(2021)

# validation_split <- validation_split(train2, prop = 0.7)
validation_split <- vfold_cv(train2, v = 10, strata = sale_price)

```

# Set the tuning spec of Elastic Net


```{r}
tune_spec <- linear_reg(penalty = tune(),
                        mixture = tune()) %>% #ridge
  set_engine("glmnet")

grid_regular(penalty(), levels = 100)

param_grid <- grid_regular(penalty(),
                           mixture(),
                           levels = list(penalty = 100,
                                         mixture = 5))
```

# Set workflow()

```{r}
workflow <- workflow() %>%
  add_model(tune_spec) %>% 
  add_formula(sale_price ~ .)
```

```{r}
library(tictoc)
doParallel::registerDoParallel()

tic()
tune_result <- workflow %>% 
  tune_grid(validation_split,
            grid = param_grid,
            metrics = metric_set(rmse))
toc()
```

```{r}
tune_result %>% 
  collect_metrics()
```

# Visualization of the tunning result

```{r}
tune_best <- tune_result %>% select_best(metric = "rmse")
tune_best$penalty
tune_best$mixture
```

```{r message=FALSE}
tune_result %>%
  collect_metrics() %>%
  filter(mixture == tune_best$mixture) %>% 
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5) +
  scale_x_log10() +
  theme(legend.position = "none") +
  labs(title = "RMSE")
```

```{r}
tune_result %>% show_best()
```


# Set Elastic net regression model and fitting

`mixture` parameter determines the proportion of Lasso regression in the Elastic net.

```{r message=FALSE, warning=FALSE}
elastic_model <- 
    linear_reg(penalty = tune_best$penalty,
               mixture = tune_best$mixture) %>%
    set_engine("glmnet")

elastic_fit <- 
    elastic_model %>% 
    fit(sale_price ~ ., data = train2)

options(max.print = 10)
elastic_fit %>% 
    tidy() %>% 
    filter(estimate > 0.001)
```

# Prediction

```{r warning=FALSE}
result <- predict(elastic_fit, test2)
result$.pred <- exp(result$.pred)
result %>% head()
```