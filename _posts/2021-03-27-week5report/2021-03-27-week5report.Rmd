---
title: "WEEK_5 Report"
description: |
  Third try to predict sale price with Multiple regression and Lasso
author:
  - name: sonsungman
    url: https://sonsungman.github.io/SKKU-Preditive.m/
date: 03-27-2021
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    draft: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
library(purrr) ; library(magrittr) ; library(MASS)
```

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
theme_set(theme_bw())
```


# Introduction


Feature selection plays an important role in solving overfitting problems. Feature selection approaches can be categorized into three broad classes: the filter methods, the wrapper methods, and embedded methods. The filter method has advantages as it can be performed independently of the model in the data preprocessing process, while the wrpper method has the potential for forward selection. Examples of embedded methods include regularization methods.


The aim of this report is to compare performance with multiple regression models, ridge regression, and lasso regression models applying regularization to improve understanding of the overfitting problem. Another approach to solving the overfitting problem, feature selection, is viewed using genetic algorithms rather than conventional Forward, Backward, and Stepwise. Additionally, adding several steps further from the previous report to the data preprocessing process is also covered.



***

A list of the books I have referenced to produce this report is shown below.

> -  Feature Engineering and Selection_ A Practical Approach for Predictive Models, Max Kuhn, Kjell Johnson(2019)
> -  An Introduction to Statistical Learning with Applications in R , Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013)
> -  Applied Predictive Modeling, Max Kuhn, Kjell Johnson (2013)
> -  R for Data Science_ Import, Tidy, Transform,  Visualize, and Model Data , Hadley Wickham, Garrett  Grolemund (2017)
> - Study Manual Exam PA, Ambrose Lo (2020 Spring)

***



# Example with Lecture


## Preparations {.tabset .tabset-fade}

### Data load


```{r, message=FALSE}
train <- read_csv(file= "train.csv")
test <- read_csv("test.csv")
```


### Data overview {.tabset .tabset-fade}

```{r}
dim(train)
dim(test)
```

We can see train doesn't have the target variable `SalePrice`.

```{r}
"SalePrice" %in% names(test)
```


### Detailed info. `train`

```{r}
skim(train)
```

### Detailed info. `test`

```{r}
skim(test)
```


```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = SalePrice)) +
  geom_histogram()
```

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = log(SalePrice))) +
  geom_histogram()
```

### `NA`s

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(naniar)
train %>% 
  # select_if(~sum(is.na(.)) > 0) %>% # alternative way
  select(where(~sum(is.na(.)) > 0)) %>% 
  gg_miss_var()
```


```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  select(where(~sum(is.na(.)) > 0)) %>% 
  gg_miss_upset()
```

### Preprecessing with `recipe`

### `all_data` combine and name cleaning with `janitor`

```{r}
all_data <- bind_rows(train, test) %>% 
  janitor::clean_names()
names(all_data)[1:10]
```

### Make recipe

```{r}
housing_recipe <- all_data %>% 
  recipe(sale_price ~ .) %>%
  step_rm(id) %>% 
  step_log(sale_price) %>% 
  step_dummy(all_nominal()) %>% 
  step_meanimpute(all_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  prep(training = all_data)

print(housing_recipe)
```

### `juice` the all_data2 and split

```{r}
all_data2 <- juice(housing_recipe)
```

We are done for preprocessing. Let's split the data set.

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]
```


```{r}
train2 %>% 
  head() %>% 
  kable()
```


## Set lasso regression model and fitting 


```{r}
  library(MASS) 

lasso_model <- 
    linear_reg(penalty = 0.01, mixture = 1) %>% # lasso: 1, ridge: 0
    set_engine("glmnet")

lasso_fit <- 
    lasso_model %>% 
    fit(sale_price ~ ., data = train2)

options(max.print = 10)
lasso_fit %>% 
    tidy() %>% 
    filter(estimate > 0.001)

```

### Ridge model

```{r}

  ridge_model <- 
    linear_reg(penalty = 0.01, mixture = 0) %>% # lasso: 1, ridge: 0
    set_engine("glmnet")

ridge_fit <- 
    ridge_model %>% 
    fit(sale_price ~ ., data = train2)

```


### Forward vs lasso vs ridge


```{r, message=FALSE, warning=FALSE, results='hide'}


    lm_model1<-lm(sale_price ~ ., data = train2)
    null_reg <-lm(sale_price ~ 1, data = train2)
    
   step_model <- null_reg %>% 
  stats::step(direction="forward", scope=list(lower=null_reg, upper=lm_model1))
    
    
```  
 


## Prediction


```{r}
result_forward <- as.tibble(predict(step_model, test2))
result_lasso <- predict(lasso_fit, test2)
result_ridge <- predict(ridge_fit, test2)

result_forward %>% head()
```

```{r}
submission <- read_csv("sample_submission.csv")
submission$SalePrice <- exp(result_forward$value)
write.csv(submission, row.names = FALSE,
          "forward_fit.csv")

submission <- read_csv("sample_submission.csv")
submission$SalePrice <- exp(result_lasso$.pred)
write.csv(submission, row.names = FALSE,
          "lasso_fit.csv")

submission <- read_csv("sample_submission.csv")
submission$SalePrice <- exp(result_ridge$.pred)
write.csv(submission, row.names = FALSE,
          "ridge_fit.csv")
```


