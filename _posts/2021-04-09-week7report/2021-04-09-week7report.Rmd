---
title: "WEEK_7 Report"
description: |
  Middle HW
author:
  - name: sonsungman
    url: https://sonsungman.github.io/SKKU-Preditive.m/
date: 04-09-2021
output:
  distill::distill_article:
    self_contained: false
    code_folding: true
    draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
library(purrr) ; library(magrittr) ; library(MASS)
```

```{r load_lib, include=FALSE}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
library(glmnet)
theme_set(theme_bw())
```

## 1. Ridge regression solution

### 1) 

Let $X = U \Sigma V^{T}$. Under the conditions given in the problem, $V^{T}V = I$ and $U^{T}U =I$.

  
$$
\begin{aligned}
 (X^{T}X + \lambda I)^{-1} X^{T}  
    &=  [(U \Sigma V^{T})^{T} U \Sigma V^{T} + \lambda I]^{-1}(U \Sigma V^{T})^{T} \\
    &=  [V\Sigma U^{T} U\Sigma V^{T} + \lambda I]^{-1}V\Sigma^{T}U^{T} \\
    &= [V\Sigma^{T}\Sigma V^{T} + \lambda I]^{-1}V\Sigma U^{T} \\
    &=[V \Sigma^{2} V^{T} + \lambda VV^{T} ]^{-1}V\Sigma U^{T} \\
    &=[V(\Sigma ^{2} + \lambda I)V^{T}]^{-1}V\Sigma U^{T} \\
    &=(V^{T})^{-1}(\Sigma ^{2} + \lambda I)^{-1}\Sigma U^{T} 
\end{aligned}
$$

$(\Sigma ^{2} + \lambda I)^{-1}\Sigma$ is a diagonal matrix with elements $\frac{\sigma_{i}}{\sigma^{2}_{i}+\lambda}$ $i = 1, ... ,d$ 

Let's call this matrix as $\Sigma^{*}$

Then,

$$
  \begin{aligned}
  (X^{T}X + \lambda I)^{-1} X^{T} 
   &= V \Sigma^{*} U^{T}
  \end{aligned}
$$


### 2)


$$
  \begin{aligned}
    \hat{\beta}_{Ridge} &= (X^{T}X + \lambda I)^{-1} X^{T}y \\
    &= V \Sigma^{*} U^{T}y
  \end{aligned}
$$


$$
  \begin{aligned}
  0 \le\|\hat{\beta}_{Ridge}\|_{2} &= \|V \Sigma^{*} U^{T}y\|_{2} \\
  &= \|V \Sigma^{*} U^{T}\|_{F}\|y\|_{2} \\
  &= \sqrt{tr(\Sigma^{*})}\|y\|_{2} \\
  &= \sqrt{\sum_{i = 1}^{d} (\frac{\sigma_{i}}{\sigma^{2}_{i}+\lambda})^{2}}\|y\|_{2}
  \end{aligned}
$$

$\|\centerdot\|_{F}$ means Frobenious norm


$$
  0\le\|\hat{\beta}_{Ridge}\|_{2}\le\min(\sqrt{\sum_{i = 1}^{d} (\frac{\sigma_{i}}{\sigma^{2}_{i}+\lambda})^{2}}\|y\|_{2},\|\hat{\beta}_{OLS}\|_{2})
$$

As the $\lambda$ grows, the upper bound of $\|\hat{\beta}_{Ridge}\|_{2}$ becomes smaller, so the value of $\|\hat{\beta}_{Ridge}\|_{2}$ becomes smaller.



## 2. Kaggle competition data

### 1) calculates the all the coefficients

#### Data load


```{r, message=FALSE}
train <- read.csv(file = "C:/Users/Actuary/Desktop/R with Stan/SKKU-Preditive.m2/_posts/2021-04-09-week7report/train.csv") %>% 
  janitor::clean_names()
```

```{r}
housing_recipe <- train %>% 
  recipe(sale_price ~ .) %>%
  step_rm(id) %>% 
  step_log(sale_price) %>%
  step_modeimpute(all_nominal()) %>% 
  step_dummy(all_nominal()) %>% 
  step_meanimpute(all_predictors()) %>%
  step_normalize(all_predictors()) %>% 
  prep(training = train)

```

```{r}
train2 <- juice(housing_recipe)

```

Cross-validation for hyperparameter tuning

```{r}
set.seed(2021)

validation_split <- vfold_cv(train2, v = 10, strata = sale_price)

```



```{r}
tune_spec_Ridge <- linear_reg(penalty = tune(), mixture = 0) %>%
  
  set_engine("glmnet")

Ridge_grid <- grid_regular(penalty(), levels = 100)

```



```{R}
tune_spec_Lasso <- linear_reg(penalty = tune(), mixture = 1) %>%
  
  set_engine("glmnet")

Lasso_grid <- grid_regular(penalty(), levels = 100)

```


```{r}
workflow_Ridge <- workflow() %>%
  add_model(tune_spec_Ridge) %>% 
  add_formula(sale_price ~ .)

workflow_Lasso <- workflow() %>%
  add_model(tune_spec_Lasso) %>% 
  add_formula(sale_price ~ .)
```


```{r}
library(tictoc)
doParallel::registerDoParallel()

tic()
tune_result_Ridge <- workflow_Ridge %>% 
  tune_grid(validation_split,
            grid = Ridge_grid,
            metrics = metric_set(rmse))
toc()
```


```{r}
library(tictoc)
doParallel::registerDoParallel()

tic()
tune_result_Lasso <- workflow_Lasso %>% 
  tune_grid(validation_split,
            grid = Lasso_grid,
            metrics = metric_set(rmse))
toc()
```

```{r}
result_R<-tune_result_Ridge %>% 
  collect_metrics()
```

```{r}
result_L<- tune_result_Lasso %>% 
  collect_metrics()
```


```{r}
  final_Ridge <- 
  workflow_Ridge %>%
  finalize_workflow(select_best(tune_result_Ridge, 'rmse')) %>%
  fit(data = train2)

  head(final_Ridge)
```

```{r}
  final_Lasso <- 
  workflow_Lasso %>%
  finalize_workflow(select_best(tune_result_Lasso, 'rmse')) %>%
  fit(data = train2)

  head(final_Lasso)

```



## 2. Example from HW

### 2) Plot the coefficients vs. the specified penalty values

This is example from HW pdf

```{r}
  Boston=na.omit(Boston)
x=model.matrix(crim~.,Boston)[,-1]
y=as.matrix(Boston$crim)
lasso.mod =glmnet(x,y, alpha =1)
beta=coef(lasso.mod)

tmp <- as.data.frame(as.matrix(beta))
tmp$coef <- row.names(tmp)
tmp <- reshape::melt(tmp, id = "coef")
tmp$variable <- as.numeric(gsub("s", "", tmp$variable))
tmp$lambda <- lasso.mod$lambda[tmp$variable+1] # extract the lambda values
tmp$norm <- apply(abs(beta[-1,]), 2, sum)[tmp$variable+1] # compute L1 norm


# x11(width = 13/2.54, height = 9/2.54)
ggplot(tmp[tmp$coef != "(Intercept)",], aes(lambda, value, color = coef, linetype = coef)) + 
    geom_line() + 
    scale_x_log10() + 
    xlab("Lambda (log scale)") + 
    guides(color = guide_legend(title = ""), 
           linetype = guide_legend(title = "")) +
    theme_bw() + 
    theme(legend.key.width = unit(3,"lines"))

```

#### Lasso

```{r}

  x <- model.matrix(sale_price ~ . , train2)[,-37]
  y <- train$sale_price
  
  train_sam <- sample(1:nrow(x), nrow(x)/2)
  test_sam <-(-train_sam)
  ytest <- y[test_sam]

  cv.lasso <- cv.glmnet(x[train_sam,], y[train_sam], alpha = 1)
  
  lasso.coef <- predict(cv.lasso, type = "coefficients", s=cv.lasso$lambda.min )
  
  plot(cv.lasso$glmnet.fit, xvar = "lambda")

```


#### Ridge

```{r}

  x <- model.matrix(sale_price ~ . , train2)[,-37]
  y <- train$sale_price
  
  train_sam <- sample(1:nrow(x), nrow(x)/2)
  test_sam <-(-train_sam)
  ytest <- y[test_sam]

  cv.ridge <- cv.glmnet(x[train_sam,], y[train_sam], alpha = 0)
  
  lasso.coef <- predict(cv.ridge , type = "coefficients", s=cv.lasso$lambda.min )
  
  plot(cv.ridge$glmnet.fit, xvar = "lambda")
  

```
